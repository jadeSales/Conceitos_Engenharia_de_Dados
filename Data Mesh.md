# Data Mesh


Referência: NEOWAY.Data Mesh: qual será o futuro da arquitetura de dados?. Disponível em: https://blog.neoway.com.br/data-mesh/. Acesso em: 28 ago 2022.

"<b>O que é Data Mesh?</b>

Data Mesh — ou malha de dados, em português — é um novo modelo de arquitetura de dados que funciona de maneira altamente descentralizada, em oposição aos Data Warehouses ou Data Lakes, que concentram todas as informações em um único repositório.

Essa abordagem é derivada do design, orientado por domínios de dados, e vai de encontro à forma como o Big Data era encarado até então, isto é, com a noção de que é necessária uma estrutura centralizada para usufruir de todo o potencial analítico da ferramenta.

O Data Mesh surge em um contexto em que as empresas precisam buscar formas para lidar com os problemas e desafios, associados à enorme escala de dados, que deve ser analisada atualmente. Nesse sentido, essa nova proposta de arquitetura de dados visa um novo modo de organizar e pensar as informações.

A grande promessa do Data Mesh é potencializar e permitir a escalabilidade da análise de dados, garantindo a acessibilidade e a disponibilidade das informações. Esses dois fatores, aliás, são indispensáveis em um cenário em que vemos a proliferação de ferramentas como Machine Learning, e do desenvolvimento de aplicações centradas em dados.

<b>Quais problemas resolve?</b>

As limitações da arquitetura tradicional de dados se mostraram um obstáculo para que as empresas possam, efetivamente, aproveitar todo o potencial das informações que têm à sua disposição, sobretudo no que se refere à otimização de processo e práticas de negócios. Em outras palavras, essa estrutura pode limitar a transformação dos dados em conhecimentos acionáveis.

Diante disso, o Data Mesh traz uma abordagem que permite corrigir algumas das lacunas deixadas pela estrutura vigente de gerenciamento de Big Data.

Primeiramente, as plataformas de dados centralizadas não possuem diversidade de fontes de informações suficientes, e as estruturas de domínio necessárias para a geração de insights a partir desses dados. Isso limita a capacidade de realizar análises mais precisas.

Além disso, na abordagem tradicional, diferentes áreas de uma mesma empresa podem lidar com conjuntos diferentes de dados, sem que haja colaboração, nem interoperabilidade (capacidade de sistemas, pessoas e organizações trabalharem em conjunto, garantindo a troca de informações de maneira eficaz e eficiente).

Com uma arquitetura complexa, as informações ficam nas mãos de poucas pessoas com grande especialização, criando gargalos no pipeline de dados. Assim, é comum que apenas partes das informações circulem de uma equipe para a outra, o que inibe o potencial de transformação, integração e entrega de dados.

Nesse mesmo sentido, os consumidores desses dados frequentemente operam com finalidades divergentes, e isso pode impactar negativamente a produtividade.

<b>Quais os princípios dessa abordagem?</b>

O Data Mesh se baseia em quatro princípios. São eles:

<b>Propriedade de domínio</b>

O Data Mesh propõe descentralizar a propriedade dos dados, movendo para diversos times distribuídos nos domínios. Isso permite utilizar as informações mais próximas às fontes, sem a necessidade de movimentá-las, o que reduz a complexidade da operação, melhora o fluxo de trabalho e reduz custos.

Com a propriedade descentralizada, não há a necessidade se manter uma única cópia dos dados; é possível ter diferentes cópias e criar um modelo que se baseia na relevância e que possibilita distribuir a responsabilidade sobre o uso das informações.

<b>Disponibilização dos dados como produto</b>

Ao distribuir a propriedade dos dados em diferentes domínios, é possível garantir um crescimento mais orgânico. Para evitar que haja problemas relacionados à padronização do acesso e da qualidade das informações, é preciso passar a pensar nos dados como produtos, e não mais somente como serviços ou ativos.

Isso inclui o surgimento de novas funções: Data Product Owner e Data Developer, que assumem a responsabilidade pela definição e desenvolvimento de produtos relacionados aos dados, com o objetivo de proporcionar uma melhor experiência para os clientes.

Quando pensamos nos dados como produtos, é necessário garantir que eles sejam acessíveis e estejam disponíveis. Isso também passa por definir como eles poderão ser acessados e por evitar quaisquer dificuldades de entendimento.

Por óbvio, dados como produtos exigem um cuidado maior com sua qualidade. Para isso, deve-se assegurar que as informações sejam oriundas de fontes confiáveis e que os processos relacionados a elas sejam rastreáveis. Isso é importante para se precaver de problemas relacionados a dados incompletos ou incorretos.

<b>Infraestrutura para disponibilizar os dados como self-service</b>

A descentralização da propriedade dos dados em domínios traz riscos relacionados ao conhecimento e tecnologia de dados, que antes eram concentrados em um só time. Isso poderia levar a um trabalho duplicado nas diferentes equipes.

Surgem, portanto, outras funções, como Data Platform Product Owner e Data Platform Engineer. Elas são responsáveis por reduzir a carga cognitiva dos times de domínio, focando na experiência do cliente e na criação de tecnologias que possam ser adotadas por todos os domínios.

Garantir a conectividade, armazenamento e processamento dos dados também é importante. Para isso, é preciso oferecer uma infraestrutura de dados pronta para ser utilizada e que assegura a interoperabilidade, o armazenamento em diferentes linguagens e um melhor fluxo de trabalho.

<b>Governança federada de dados</b> 

A partir dos princípios anteriores, os domínios podem utilizar a plataforma de autosserviço de dados para criar seus produtos. No entanto, times distribuídos, mas que atuam de forma isolada, não são o suficiente para extrair o máximo potencial das informações. Novamente, portanto, é necessário que se tenha interoperabilidade.

A governança federada trata da segurança e precisão dos dados, evitando que eles sejam utilizados de maneira indevida. Isso passa pelo compartilhamento da responsabilidade pela definição das regras globais para uso das informações.

Determinar a sintaxe e a semântica dos dados também é importante, criando padrões e protocolos que façam com que eles sejam, de fato, interoperáveis; inclusive com interfaces comuns para acessá-los pela plataforma de autosserviço.

<b>Quais os benefícios do Data Mesh?</b>

Por fim, não podemos deixar de falar dos benefícios do Data Mesh para as empresas. Destacamos dois deles:

Agilidade e escalabilidade: por conta das operações descentralizadas e da melhor infraestrutura de dados, é possível escalar as análises e tornar os processos mais ágeis. Isso se reflete, por exemplo, na redução do tempo de chegada ao mercado de uma nova solução.

Governança forte: o Data Mesh permite um maior controle sobre a conformidade dos dados de ponta a ponta. Isso é fundamental em um cenário com cada vez mais fontes e formatos de informação. O resultado são dados mais acessíveis e com maior qualidade, o que se reflete em melhores análises."

------------------------------------------------------------------------------------
<br>
Referência: .ZUP. Data Mesh: um (não tão) novo paradigma para entregar valor com dados. Disponível em: https://www.zup.com.br/blog/data-mesh. Acesso em: 28 ago 2022. 

<br>


"Olá, Data Mesh!
O processo criativo não está limitado à criação de novos elementos, mas também à combinação de elementos existentes para resolver novos problemas. No Data Mesh, o novo não está na criação de tecnologias e/ou ferramentas, mas sim na combinação de conceitos e práticas consolidadas, criando uma nova abordagem para trabalhar com dados:

Data: pensar dados como cidadãos de primeira classe, como parte essencial da estratégia executiva e tecnológica. Podemos observar esta tendência na relação entre a capacidade das empresas usarem dados como diferenciais competitivos e seus resultados no mercado;

Product thinking: pensar dados como produtos, não como projeto ou serviço, oferecendo uma ótima experiência para os usuários. Podemos observar esta prática se consolidando em produtos com funcionalidades incríveis, cada vez mais simples de usar;

Distributed domain driven design architecture: pensar dados como parte dos domínios de negócio, distribuindo efetivamente a autonomia e responsabilidade. Podemos observar esta nova arquitetura ganhando espaço junto com Domain-driven design (DDD), microservices e service mesh;

Self-service platform design: reduzir a carga cognitiva dos usuários com padrões, protocolos, tecnologias e ferramentas agnósticas de domínio, disponíveis em uma plataforma de autosserviço. Podemos observar estas plataformas sendo possíveis com o uso intensivo de containers e aumento da oferta e redução de custos da nuvem.

<b>Princípios do Data Mesh</b>

Com a fundação da nova arquitetura definida, podemos entender como construir as paredes, através de 4 princípios essenciais.

- Princípio 1 – Domain Ownership

O primeiro e mais importante princípio do Data Mesh é mover a propriedade dos dados de um único time centralizado para muitos times distribuídos nos domínios de negócio. Ao fazer isso, podemos observar algumas vantagens, incluindo:

Usar os dados perto das fontes, ao invés de movimentar dados: movimentar os dados tem dois custos relevantes. Primeiro, complexidade de operação, caso seja necessário adicionar mais jobs de processamento no workflow de dados, e assumindo que cada novo job é um potencial ponto de falha. Segundo e mais importante, a semântica (significado) dos dados tender a diminuir nestas movimentações, principalmente na movimentação entre times;

Usar a cópia mais relevante, ao invés de tentar manter uma única cópia: o esforço necessário para manter uma única cópia dos dados é maior do que o resultado, sendo comum existir uma falsa sensação de controle, enquanto as pessoas continuam criando suas cópias “fora do radar”. Mover o esforço dos processos de controle e auditoria para criar um modelo de relevância e gerenciar melhor o ciclo de vida dos dados permite usar os próprios clientes e times de governança como reguladores orgânicos, distribuindo também a responsabilidade;

Dados distribuídos por domínios, ao invés de partições técnicas: ao distribuir os dados por domínio, balanceamos a gravidade dos dados, que antes ficavam concentrados em alguns times, normalmente organizados por estágios do processo de ETL. Ao balancear a gravidade dos dados, temos um modelo de escala orgânica, alinhados com o padrão de crescimento de negócio, respondendo melhor à proliferação de novas fontes de dados e casos de uso.

No final do dia, este princípio é essencial para escalar no mesmo ritmo de crescimento das fontes e de quem consome os dados.

- Princípio 2 – Data as Product

Distribuir a propriedade dos dados nos domínios é uma ideia interessante e habilita um modelo de crescimento mais orgânico. Por outro lado, algumas garantias do modelo centralizado são perdidas, por exemplo, a padronização no acesso e qualidade dos dados. 

Para equilibrar o modelo distribuído e a necessidade de dados acessíveis e com qualidade, é necessário pensar nos dados como produtos, incluindo:

Novas funções, Data Product Owner e Data Developer: duas novas funções nascem com o modelo distribuído, para assumir a responsabilidade pela definição e desenvolvimento dos produtos de dados. Ao invés de pensar nos dados apenas como ativos ou como serviços, Data Product Owners aplicam o pensamento de produto para criar a melhor experiência para clientes, enquanto Data Developers trabalham com foco em desenvolver o produto:

Dados acessíveis, documentados e disponíveis: ao pensar nos dados como produtos, as pessoas responsáveis pensam mais na experiência dos usuários, incluindo definir como os dados serão acessados (como uma tabela em um Data Warehouse ou como um streaming de dados real-time), documentar melhor os dados, incluindo sintaxe e semântica, tendo em vista que qualquer dificuldade para entender os dados agora são de responsabilidade do próprio time, e não de um time terceiro;

Dados confiáveis, verificados e rastreáveis: ainda, ao pensar nos dados como produtos,  as pessoas responsáveis não precisam apenas garantir que os dados estejam acessíveis, documentados e disponíveis, mas também garantir a qualidade destes dados. Esta qualidade inclui garantir que os dados venham de fontes verificadas e que todos os processos relacionados aos dados sejam rastreáveis, evitando problemas com dados incompletos e/ou incorretos.

No final do dia, este princípio é sobre oferecer uma ótima experiência para que os usuários possam descobrir, entender e usar dados de alta qualidade.

- Princípio 3 – Self-service Data Platform

Ao distribuir a propriedade dos dados entre os domínios, existe o risco de pulverizar todo o conhecimento, habilidade e tecnologias de dados, antes concentrada em um único time. 

Esta pulverização aumentaria a carga cognitiva dos times de domínio e criaria muito trabalho duplicado. Neste cenário, pensar no time centralizado não mais como um monolito, mas como uma plataforma de dados agnóstica de domínio, incluindo:

Novas funções, Data Platform Product Owner e Data Platform Engineer: estas novas funções são responsáveis não mais pelos dados específicos de domínio, mas têm um novo objetivo: oferecer a melhor experiência para criar produtos de dados, reduzindo a carga cognitiva dos times de domínio. Data Platform Product Owners identificam e criam jornadas com foco no cliente, enquanto Data Platform Engineers usam seu conhecimento e habilidade para criar tecnologias que possam ser usadas por todos os domínios;

Plano de desenvolvimento, criar, versionar e publicar produtos de dados: oferecer a melhor experiência para Data Developers do domínio criarem e publicarem seus produtos de dados, incluindo stacks de processamento de dados, pipelines para integração e entrega contínua;

Plano de infraestrutura, conectividade, armazenamento, processamento: reduzir a carga cognitiva de Data Developers, oferecendo infraestrutura de dados pronta para usar. Infraestrutura de dados inclui: conectividade de rede garantindo que nenhum Data Developer precise configurar redes, rotas ou firewall, armazenamento poliglota como storage de objetos, tabelas e streaming de dados, e infraestrutura para processamento de jobs e criação de workflows tolerantes à falhas.

No final do dia, este princípio é sobre garantir que os times dos domínios possam criar e consumir produtos de dados com autonomia, usando as abstrações da plataforma.

- Princípio 4 – Federated Computational Governance

Aplicando os princípios anteriores, os times dos domínios usam uma plataforma de dados de autosserviço para criar seus produtos de dados. Mesmo equilibrando autonomia e responsabilidade, em um modelo de produtos e times distribuídos mas isolados, não é possível extrair o máximo valor dos dados, é necessário ter interoperabilidade. O desafio é criar um modelo de governança federada, que equilibre descentralização e centralização, incluindo:

Adotar federação, compartilhar a responsabilidade: tradicionalmente, times de governança usam um modelo centralizado de regras e processos, e acumulam toda a responsabilidade para garantir padrões globais para os dados. Neste novo modelo, o time de governança muda sua abordagem para compartilhar a responsabilidade através de federação, sendo responsável por definir, por exemplo, quais são as regras globais (não locais) para qualidade e segurança de dados, ao invés de ser sozinho responsável pela qualidade e segurança de todos os dados da empresa. Pensando em LGPD, o time de governança global continua sendo o responsável legal e pode inspecionar os domínios para garantir as regras globais;

Definir sintaxe e semântica: definir modelos globais para identificar e dar significado aos dados, incluindo padrões para nomes de tabelas e colunas, taxonomia, elementos de metadados e políticas de controle de acesso;

Definir padrões e protocolos de interoperabilidade: criar padrões e protocolos para garantir a interoperabilidade, definindo interfaces comuns para acessar os dados, usando formatos de armazenamento e compartilhamento de dados abertos, embutidos na plataforma de autosserviço, sem a necessidade de processos manuais.

No final do dia, este princípio é sobre permitir que os usuários obtenham valor a partir de interoperabilidade entre produtos de dados, seguindo padrões e protocolos globais."

------------------------------------------------------------------------------------
<br>
Referência: SAVIO TELES. Data Mesh: indo além do Data Lake e Data Warehouse. Disponível em: https://medium.com/data-hackers/data-mesh-indo-al%C3%A9m-do-data-lake-e-data-warehouse-465d57539d89. Acesso em: 28 ago 2022.

<br>

"Data Mesh: indo além do Data Lake e Data Warehouse

Na Era da Informação, o poder dos dados dentro das empresas tem sido mantido nas mãos de poucas pessoas, geralmente nos departamentos de TI. As unidades de negócio, como marketing, analistas de negócio e executivos usam os dados para tomarem decisões importantes, mas geralmente tem que requisitar estes dados ao departamento de TI. Esta situação centraliza a coleta e transformação dos dados nas mãos de equipes que, em muitos casos, estão desalinhadas das necessidades das áreas de negócio da empresa. Por exemplo, imagine uma empresa de e-commerce com dados de clientes e pedidos (realizados no site) que são utilizados por várias áreas da organização:

A área de marketing necessita visualizar o volume de vendas por categorias de produtos para expandir suas campanhas, dinamicamente todos os dias.
A área de ciência de dados está construindo o sistema de recomendação de produtos e precisam ter os dados atualizados para o treinamento dos modelos.
A área de gestão da empresa precisa visualizar os resultados agregados de cada área e de toda a empresa.
O problema é que todas as demandas de coleta e preparação dos dados para estas áreas da empresa ficam, geralmente, concentradas na área de TI, onde um time centralizado de engenheiros de dados precisam preparar os dados para as área de negócio. Entretanto, em muitos casos, os engenheiros de dados não tem o conhecimento de negócio necessário para gerar os dados com a qualidade esperada pela área de negócio. Além disso, a centralização desta tarefa nas mãos dos engenheiros de dados, desconectados das outras áreas, leva a menor velocidade de entrega das áreas de negócio. Quem nunca ouviu uma área de negócio reclamar que precisa analisar alguma informação importante (já requisitada à engenharia), mas que a engenharia está demorando muito pra entregar.

Muitas empresas estão investindo na próxima geração do seu Data Lake/Warehouse, com a esperança de democratizar os dados em larga escala para fornecer insights de negócios e, por fim, tomar decisões inteligentes automatizadas. Embora os avanços tecnológicos da última década com Data Lake/Warehouse tenham lidado com a escala em relação ao volume e processamento de dados, eles falharam em lidar com a escala em outras dimensões: mudanças no cenário de dados, aumento do número de fontes de dados, diversidade de regras de negócio sobre os dados e velocidade de resposta à mudanças nas regras de negócio.

As plataformas de dados baseadas na arquitetura Data Lake/Warehouse têm características comuns que levaram a promessas não cumpridas na democratização dos dados em larga escala dentro das empresas:

- centralizada e monolítica: um time de engenharia é responsável por todo o pipeline de dados, desde a coleta dos dados na fonte até a disponibilização das informações aos usuários de negócio. Este time se torna o gargalo quando é necessária a alteração do pipeline de dados. No exemplo do e-commerce, um único time da engenharia centralizaria a responsabilidade por preparar os dados para várias áreas diferentes da empresa. Só que cada área possui requisitos diferentes sobre os dados de clientes e pedidos e, geralmente, a equipe de engenharia não consegue ter uma visão de negócio especializada para todas as áreas e dar celeridade em resposta às mudanças nas regras de negócio.

- plataforma de dados centralizada: os arquitetos de dados implementam um pipeline de dados com vários estágios (ingestão, preparação, agregação, etc.). Apesar deste modelo fornecer algum nível de escala, alocando times diferentes para cada estágio do pipeline, ele tem uma limitação inerente que retarda a entrega de novas funcionalidades, pois um estágio é dependente do outro. Existe um forte acoplamento entre os estágios do pipeline para entregar uma funcionalidade independente. O estágio de transformação de dados, por exemplo, depende do estágio de extração de dados, que pode estar sob a responsabilidade de uma equipe diferente.

- grupo de engenheiros de dados altamente especializados trabalhando em silos: engenheiros de dados trabalham, geralmente, isolados do restante da empresa, ou seja, distantes de onde os dados são gerados (fontes de dados) e utilizados (áreas onde as decisões são tomadas sobre os dados). Os engenheiros não são separados apenas organizacionalmente, mas também separados e agrupados em times baseados nas suas expertises em ferramentas de big data, geralmente sem o conhecimento necessário da área de negócio. Suponha, no exemplo do e-commerce, que a área de marketing esteja requisitando que sejam coletados dados do Twitter para analisar o engajamento dos consumidores com a empresa. Neste caso, um time estaria focado em trazer os dados do Twitter, outro time de engenheiros em preparar estes dados e o time de marketing em analisar as informações. O problema é que os times de engenharia que coletam e preparam estão isolados e ficam, geralmente, muito longe da área de negócio, podendo não entregar o resultado esperado.

Esta estrutura centralizada do Data Lake/Warehouse trouxe alguns desafios às equipes para democratização dos dados dentro da empresa:

- Falta dos responsáveis pelos dados: quem são os responsáveis pelos dados?
- Problemas de qualidade dos dados: o time de infraestrutura é responsável pela qualidade, mas não conhece os dados tão bem, pois não estão intimamente ligados com o time de negócio.
- Escalabilidade organizacional: o time centralizado de ETL se torna o gargalo na democratização dos dados na empresa.

<b> O que é Data Mesh?</b>

O Data Mesh foi proposto por Zhamak Dehghani (diretora de tecnologia na ThoughtWorks) no artigo “How to Move Beyond a Monolithic Data Lake to a Distributed Data Mesh” como uma nova abordagem para projetar e desenvolver arquiteturas de dados com o objetivo de facilitar a democratização em escala dos dados na empresa. Ao contrário de arquiteturas centralizadas e monolíticas baseados em um data warehouse (armazém de dados) e/ou um data lake (lago de dados), um data mesh é um paradigma arquitetural e organizacional que desafia a antiga suposição de que devemos centralizar grandes volumes de dados analíticos para usá-los, manter todos os dados em um só lugar ou gerenciá-los por meio de um time de dados centralizado para entregar valor às áreas de negócio.

O Data Mesh (Malha de Dados) segue quatro princípios, que segundo Zhamak, são necessários para qualquer implementação do data mesh atingir os resultados esperados de escala, qualidade e integridade dos dados: (1) arquitetura de dados descentralizada orientada ao domínio; (2) dados disponibilizados como produto; (3) infraestrutura para disponibilizar os dados como self-service; (4) governança federada para permitir a interoperabilidade dos domínios. Cada princípio leva a uma nova visão lógica da arquitetura técnica e da estrutura organizacional.

<b>Arquitetura de dados descentralizada orientada ao domínio</b>

A arquitetura deve ser modelada para organizar os dados analíticos por domínios. Nesta arquitetura, a interface do domínio com o restante da organização não inclui apenas os recursos operacionais, mas também o acesso aos dados analíticos que o domínio atende. No exemplo do e-commerce, poderíamos ter domínios de ‘clientes’ e ‘pedidos’. O domínio de ‘clientes’ poderia fornecer, por exemplo, endpoints de APIs para: 1) obter todos os clientes (um por linha) baseado em filtros; 2) disponibilizar estatísticas sobre os clientes (como número de clientes ativos, inadimplentes, etc.).

Cada domínio pode expor uma ou mais APIs operacionais, assim como um ou mais endpoints para serem utilizados em várias áreas da empresa. Isso implica que a arquitetura deve remover qualquer acoplamento para permitir que os domínios forneçam seus dados analíticos e permita que a engenharia concentre em gerenciar os dados, independentemente de outros domínios. Para escalar, a arquitetura deve oferecer suporte à autonomia das equipes de domínio em relação à implantação de seus sistemas de dados operacionais ou analíticos.

<b>Dados disponibilizados como produto</b>

O objetivo do Data Mesh é tratar os dados como um produto, onde cada fonte de dados tem seu próprio gerente/proprietário do produto de dados que fazem parte de uma equipe multifuncional de engenheiros de dados. Cada fonte de dados tem uma equipe focada em uma oferta autônoma, construindo uma arquitetura distribuída orientada ao domínio. Cada domínio deve ser descoberto, endereçável, auto descritivo, seguro (governado pelo controle de acesso global), confiável e interoperável (governado por um padrão aberto). Cada domínio poderá armazenar seus dados em um data lake/warehouse e, em muitos casos, também terá uma cópia de alguns dos dados em um banco de dados relacional transacional.

Cada domínio deve ter um proprietário do produto de dados, responsável por garantir que os dados sejam entregues como um produto. Estas entregas devem incluir: qualidade de dados, menor tempo de espera de consumo de dados e, em geral, satisfação do usuário de dados. O proprietário do produto deve ter um conhecimento profundo de quem são os usuários dos dados, como eles usam estes dados e quais são os métodos nativos com os quais eles se sentem confortáveis ​​para consumir os dados. Esse conhecimento íntimo dos usuários resulta no projeto de interfaces de produtos de dados que atendam as necessidades das áreas de negócio. A conversa entre os usuários e os proprietários dos produtos é uma parte necessária para estabelecer as interfaces dos produtos de dados.

<b>Plataforma de dados self-service</b>

Você deve estar pensando: “Mas, vou precisar construir uma infraestrutura de plataforma de dados para cada domínio?”. Como você pode imaginar, para construir, implantar, executar, monitorar e acessar um domínio de dados existe uma boa parte da infraestrutura que precisa ser provisionada e gerenciada. As habilidades necessárias para provisionar essa infraestrutura são especializadas e seriam difíceis de replicar em cada domínio. Mais importante ainda, a única maneira de as equipes serem proprietárias autônomas de seus produtos de dados é ter acesso a uma abstração de alto nível da infraestrutura que remove a complexidade e o desafio do provisionamento e do gerenciamento do ciclo de vida dos produtos de dados.

Isso exige que exista uma infraestrutura de dados self-service, como uma plataforma, para habilitar a autonomia do domínio. A ideia principal é evitar a duplicação de esforços, permitindo que cada equipe de produtos de dados construa seus produtos de dados rapidamente. Note que esta plataforma de infraestrutura de dados não deve se tornar uma plataforma de dados (ela permanece agnóstica em relação ao domínio). A infraestrutura self-service deve incluir recursos para reduzir o custo atual e a especialização necessária para construir produtos de dados, incluindo: armazenamento de dados escalável, esquema de produtos de dados, construção e orquestração de pipeline de dados, linhagem de dados, etc. Portanto, quando for criar uma nova API ou endpoint, os desenvolvedores podem aproveitar toda a infraestrutura pronta para construir a solução. Por exemplo, se um produto de dados precisar utilizar filas distribuídas para comunicação entre módulos da solução, os engenheiros podem aproveitar uma infraestrutura implantada com Apache Kafka e apenas criar os tópicos de comunicação entre os módulos, ao invés de precisar implantar e administrar o cluster do Kafka.

<b>Governança Federada</b>

A implementação do data mesh requer um modelo de governança de dados que abrange a descentralização do domínio, interoperabilidade por meio de padronização global, uma topologia dinâmica e, o mais importante, a execução automatizada de decisões pela plataforma. A isso denominamos governança federada. Cada dono de um produto de dados tem autonomia e poder de decisão local de domínio, enquanto cria e adere a um conjunto de regras globais — regras aplicadas a todos os produtos de dados e suas interfaces — para garantir um ecossistema saudável e interoperável. O grupo tem uma tarefa difícil: manter um equilíbrio entre centralização e descentralização; quais decisões precisam ser localizadas para cada domínio e quais decisões devem ser feitas globalmente para todos os domínios. Em última análise, as decisões globais têm um propósito, criar interoperabilidade e um efeito de rede composto por meio da descoberta e composição de produtos de dados.

O modelo de dados de domínio é uma preocupação que deve ser localizada em um domínio que esteja mais intimamente familiarizado com ele. No exemplo de e-commerce, o modelo de dados de ‘produtos mais comprados no mês’ deve ser deixada para a equipe de ‘domínio de produtos’. No entanto, a decisão sobre como identificar um ‘cliente ativo da empresa’ é uma preocupação global, pois pode ser encontrado em outros domínios. Ou seja, a governança de dados sobre o domínio de ‘cliente ativo da empresa’ deve ser centralizada em um único lugar.

Um conjunto de dados de domínio só se torna um produto de dados depois que localmente, dentro do domínio, passa pelo processo de garantia de qualidade de acordo com as métricas de qualidade do produto de dados esperadas e as regras de padronização global. Os proprietários de produtos de dados de domínio estão em melhor posição para decidir como medir a qualidade dos dados de seu domínio, conhecendo os detalhes das operações de domínio que produzem os dados. Apesar de tal tomada de decisão ser localizada e autônoma, é preciso garantir que a modelagem esteja atendendo aos padrões globais da empresa de qualidade, definido pela equipe de governança federada e automatizado pela plataforma.

<b>Arquitetura Data Mesh em nuvem</b>

Zhamak acredita que a tecnologia não seja a limitação para migração para o Data Mesh, já que as ferramentas utilizadas no Data Lake/Warehouse podem ser aproveitadas no Data Mesh. O uso das soluções em nuvem irá facilitar muito a migração da sua estrutura atual para Data Mesh. Zhamak apresenta como exemplo utilizar o Google Cloud Dataflow para o processamento de dados em streaming e em lote e o Google Cloud Data Catalog para descoberta central, controle de acesso e governança de conjuntos de dados de domínio distribuídos. Além disso, existe uma ampla variedade de opções de armazenamento de dados em nuvem que permitem que os produtos de dados de domínio escolham o armazenamento adequado para a necessidade.

O artigo “How to Create a Modern CPG Data Architecture with Data Mesh” apresenta uma arquitetura Data Mesh na AWS para gerenciar os dados da área de varejo. O setor de e-commerce cresceu 75% em meio à pandemia e este crescimento no volume de dados trouxe desafios ao setor de varejo no gerenciamento destes dados. Historicamente, o setor não se comunica diretamente com os consumidores e, por isso, os dados de clientes e compras representam praticamente a única informação disponível.

A arquitetura (apresentada na Figura abaixo) é uma solução que segue o seguinte fluxo:

1. Os dados são ingeridos dentro da AWS;
2. Os dados são gerenciados pelo domínio de dados do negócio em um modelo pub/sub onde os produtores e consumidores utilizam uma infraestrutura de comunicação, segurança, governança e descoberta;
3. Os metadados são gerenciados por múltiplos serviços: AWS Glue é utilizado para o catálogo de dados, Amazon Neptune é utilizado para armazenar a linhagem dos dados e o Amazon DynamoDB para armazenar os contratos de dados.
4. Os consumidores buscam os dados seguindo as propriedades dos contratos armazenados no DynamoDB. Estes contratos desacoplam os produtores e consumidores e permite a interação entre eles.
5. O AWS Lake Formation facilita a configuração de um data lake seguro com um repositório centralizado, administrado e seguro que armazena todos os dados, tanto em sua forma original quanto preparados para análise. Ele permite romper os silos de dados e combinar diferentes tipos de análises para obter insights e orientar as melhores decisões de negócios.
6. O processamento de análises dos dados é realizado com o Lake Formation, juntamente com Amazon Athena para consultas interativas, facilitando a análise de dados armazenados no Amazon S3 usando SQL padrão. O Elasticsearch é utilizado como mecanismo de busca e análise de dados distribuído e o Amazon SageMaker como um serviço para ajudar cientistas e engenheiros de dados a preparar, criar, treinar e implantar modelos de machine learning (ML) de alta qualidade rapidamente.

<b>Quando começar a pensar sobre Data Mesh na sua empresa?</b>

Quando você deve considerar a mudança da estrutura atual para o data mesh? Em primeiro lugar, se você está feliz com sua estrutura, se está satisfeito com a maneira como sua empresa usa os dados para tomar decisões, então não o faça! O Data Mesh exige que você construa uma arquitetura e infraestrutura que pode ser complexa para a sua empresa, dada as habilidades da sua equipe e estágio das tecnologias. Além disso (e talvez mais importante), o Data Mesh exige uma mudança de cultura dentro da sua empresa, desde a área de negócio até a engenharia, o que pode ser uma barreira na implantação desta arquitetura.

Quando o número de fontes de dados, conjunto de consumidores, regras de negócio e complexidade dos domínios for aumentando, o uso do Data Lake/Warehouse pode se tornar um gargalo na entrega de soluções de qualidade. Neste caso, talvez seja o momento de você iniciar a discussão interna na empresa sobre a migração da estrutura atual para o Data Mesh. Se você tem desenvolvimento orientado ao domínio, começou a trabalhar com microsserviços ou se fez uma migração para a nuvem, a migração para o Data Mesh tende a ter um custo bem menor.

Uma boa estratégia é utilizar estudos de casos dentro da sua empresa como “candidatos perfeitos” para serem atendidos individualmente e migrar sua arquitetura atual aos poucos para o Data Mesh. Por exemplo, se você está descartando fontes de dados a serem importadas, que são valiosas para usuários do negócio, porque é complexo integrar estas fontes de dados na estrutura de Data Lake/Warehouse atual da sua empresa, então pode ser uma boa oportunidade para utilizar este cenário como estudo de caso para migração para esta nova arquitetura."

------------------------------------------------------------------------------------
# Vídeos

Referência: Thoughtworks Brasil.[XConf Brasil 2020] Data mesh. Disponível em: https://www.youtube.com/watch?v=3qoG8KR1nwc&t=144s. Acesso em: 28 ago 2022.

Referência: Maestria Dados. O que é Data Mesh?. Disponível em: https://www.youtube.com/watch?v=JLCepomFEzw. Acesso em: 28 ago 2022.

Referência: BlueShift Brasil. #08 Bluetalks - Principais Conceitos de Data Mesh. Disponível em: https://www.youtube.com/watch?v=aU5edMne638. Acesso em: 28 ago 2022.
